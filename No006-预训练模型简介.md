# No005-预训练模型基础：利用现有知识快速构建AI应用

## 1. 预训练模型概述

预训练模型是AI领域的一项重大突破，它们通过在大规模数据集上预先训练，获取了丰富的通用知识，可以快速应用到各种下游任务中。本教程将介绍预训练模型的基本概念、优势以及如何在自己的项目中使用它们。

## 2. 什么是预训练模型

### 理论知识点
预训练模型（Pretrained Model）是指在大规模数据集上预先训练好的深度学习模型，它已经学习了丰富的特征表示和通用知识。预训练模型的主要思想是：

1. **知识迁移**：将从一个任务中学到的知识迁移到另一个相关任务
2. **减少数据需求**：预训练模型已经包含了大量通用知识，因此在下游任务上通常只需要少量标注数据
3. **提高性能**：在许多任务上，使用预训练模型作为起点通常比从头训练模型能够获得更好的性能
4. **节省计算资源**：预训练过程通常需要大量计算资源，使用已有的预训练模型可以避免重复这一过程

### 实践示例：了解预训练模型的结构

```python
# 简单示例：了解预训练模型的基本结构

# 注意：实际使用预训练模型时，我们通常使用成熟的库如transformers、torchvision等

class SimplePretrainedModel:
    def __init__(self, pretrained_weights=None):
        # 模拟预训练模型的结构
        self.feature_extractor = self._create_feature_extractor()  # 特征提取器部分
        self.classifier = self._create_classifier()  # 分类器部分
        
        # 如果提供了预训练权重，则加载
        if pretrained_weights:
            self._load_pretrained_weights(pretrained_weights)
    
    def _create_feature_extractor(self):
        # 模拟创建特征提取器（通常是较深的神经网络）
        print("创建特征提取器...")
        return "pretrained_feature_extractor"
    
    def _create_classifier(self):
        # 模拟创建分类器（通常是较浅的神经网络）
        print("创建分类器...")
        return "task_specific_classifier"
    
    def _load_pretrained_weights(self, weights):
        # 模拟加载预训练权重
        print(f"加载预训练权重: {weights}")
    
    def fine_tune(self, task_data):
        # 模拟微调过程
        print(f"在任务数据上微调模型: {task_data}")
        print("通常只更新分类器部分的权重，或对特征提取器进行部分更新")
    
    def predict(self, input_data):
        # 模拟预测过程
        print(f"使用模型进行预测: {input_data}")
        return "prediction_result"

# 模拟预训练模型的使用流程
if __name__ == "__main__":
    # 1. 创建预训练模型实例
    print("=== 预训练模型使用流程 ===")
    model = SimplePretrainedModel(pretrained_weights="large_dataset_weights")
    
    # 2. 在特定任务上微调模型
    print("\n2. 在特定任务上微调模型:")
    model.fine_tune("task_specific_dataset")
    
    # 3. 使用微调后的模型进行预测
    print("\n3. 使用微调后的模型进行预测:")
    result = model.predict("new_input_data")
    print(f"预测结果: {result}")
```

## 3. 预训练模型的优势和应用场景

### 理论知识点
预训练模型在以下场景中特别有优势：

1. **数据量有限的场景**：当你只有少量标注数据时，预训练模型可以提供很好的起点
2. **计算资源有限的场景**：从头训练大型模型需要大量计算资源，使用预训练模型可以大幅减少计算需求
3. **需要快速开发原型的场景**：预训练模型可以帮助你快速构建和验证AI应用的原型
4. **复杂的AI任务**：对于自然语言处理、计算机视觉等复杂任务，预训练模型通常比自定义模型表现更好

常见的预训练模型应用领域包括：
- 自然语言处理（NLP）：文本分类、情感分析、机器翻译等
- 计算机视觉（CV）：图像分类、物体检测、图像分割等
- 语音识别：语音转文本、说话人识别等
- 多模态任务：图文理解、视频分析等

### 实践示例：比较从头训练和使用预训练模型

```python
# 简单模拟：比较从头训练和使用预训练模型的差异

import time
import random
import matplotlib.pyplot as plt

# 设置中文显示
plt.rcParams["font.family"] = ["SimHei", "WenQuanYi Micro Hei", "Heiti TC"]
plt.rcParams["axes.unicode_minus"] = False

class ModelTrainer:
    def train_from_scratch(self, dataset_size):
        # 模拟从头训练模型
        print(f"从头训练模型，数据集大小: {dataset_size}")
        # 假设训练时间与数据集大小和模型复杂度成正比
        training_time = dataset_size * 0.1  # 模拟训练时间
        performance = min(0.9, 0.5 + dataset_size * 0.0004)  # 模拟性能上限
        
        # 模拟训练过程中的时间流逝
        time.sleep(0.5)  # 仅为了演示效果
        
        return training_time, performance
    
    def train_with_pretrained(self, dataset_size):
        # 模拟使用预训练模型
        print(f"使用预训练模型，数据集大小: {dataset_size}")
        # 预训练模型通常训练时间更短，且在小数据集上性能更好
        training_time = dataset_size * 0.03  # 更短的训练时间
        # 即使在小数据集上也能获得不错的性能
        performance = min(0.95, 0.8 + dataset_size * 0.0002)  # 更高的起点
        
        # 模拟训练过程中的时间流逝
        time.sleep(0.3)  # 仅为了演示效果
        
        return training_time, performance

# 比较不同数据集大小下的性能差异
def compare_training_methods():
    trainer = ModelTrainer()
    dataset_sizes = [100, 500, 1000, 5000, 10000]
    scratch_times = []
    scratch_performances = []
    pretrained_times = []
    pretrained_performances = []
    
    print("=== 从头训练 vs 使用预训练模型 ===")
    
    for size in dataset_sizes:
        print(f"\n测试数据集大小: {size}")
        
        # 从头训练
        print("1. 从头训练:")
        scratch_time, scratch_perf = trainer.train_from_scratch(size)
        scratch_times.append(scratch_time)
        scratch_performances.append(scratch_perf)
        print(f"   训练时间: {scratch_time:.2f}小时, 性能: {scratch_perf:.4f}")
        
        # 使用预训练模型
        print("2. 使用预训练模型:")
        pretrained_time, pretrained_perf = trainer.train_with_pretrained(size)
        pretrained_times.append(pretrained_time)
        pretrained_performances.append(pretrained_perf)
        print(f"   训练时间: {pretrained_time:.2f}小时, 性能: {pretrained_perf:.4f}")
    
    # 可视化结果
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
    
    # 训练时间对比
    ax1.plot(dataset_sizes, scratch_times, 'o-', label='从头训练')
    ax1.plot(dataset_sizes, pretrained_times, 's-', label='使用预训练模型')
    ax1.set_title('训练时间对比')
    ax1.set_xlabel('数据集大小')
    ax1.set_ylabel('训练时间 (小时)')
    ax1.grid(True)
    ax1.legend()
    
    # 性能对比
    ax2.plot(dataset_sizes, scratch_performances, 'o-', label='从头训练')
    ax2.plot(dataset_sizes, pretrained_performances, 's-', label='使用预训练模型')
    ax2.set_title('模型性能对比')
    ax2.set_xlabel('数据集大小')
    ax2.set_ylabel('模型性能')
    ax2.grid(True)
    ax2.legend()
    
    plt.tight_layout()
    plt.show()
    
    print("\n=== 比较结论 ===")
    print("1. 使用预训练模型通常可以节省大量训练时间")
    print("2. 在小数据集上，预训练模型的性能优势尤为明显")
    print("3. 随着数据集增大，两种方法的性能差异会逐渐缩小")

# 运行比较
def main():
    compare_training_methods()

if __name__ == "__main__":
    main()
```

## 4. 常见的预训练模型

### 理论知识点
随着AI技术的发展，出现了许多优秀的预训练模型。以下是一些最具代表性的预训练模型：

1. **自然语言处理领域**：
   - **BERT**：由Google开发的双向Transformer模型，彻底改变了NLP领域
   - **GPT系列**：由OpenAI开发的生成式预训练Transformer模型
   - **RoBERTa**：Facebook提出的BERT改进版本
   - **T5**：Google提出的将所有NLP任务都转化为文本生成的模型
   - **ALBERT**：BERT的轻量级变体，参数更少但性能相当

2. **计算机视觉领域**：
   - **VGGNet**：由牛津大学提出的深度卷积神经网络
   - **ResNet**：微软提出的残差网络，解决了深度网络训练难题
   - **Inception/GoogLeNet**：Google提出的多分支网络结构
   - **EfficientNet**：Google提出的高效网络架构
   - **Vision Transformer (ViT)**：将Transformer应用于计算机视觉的模型

3. **多模态领域**：
   - **CLIP**：OpenAI提出的连接文本和图像的多模态模型
   - **DALL-E**：OpenAI提出的文本到图像生成模型
   - **ALIGN**：Google提出的大规模视觉-语言预训练模型

### 实践示例：使用torchvision加载和查看预训练模型

```python
# 使用torchvision加载和查看预训练模型

import torch
import torchvision.models as models
import matplotlib.pyplot as plt

# 设置中文显示
plt.rcParams["font.family"] = ["SimHei", "WenQuanYi Micro Hei", "Heiti TC"]
plt.rcParams["axes.unicode_minus"] = False

# 列出torchvision中可用的预训练模型
def list_available_pretrained_models():
    print("=== torchvision中可用的预训练模型 ===")
    # 这些是torchvision中常用的预训练模型
    available_models = [
        "alexnet", "vgg11", "vgg13", "vgg16", "vgg19",
        "resnet18", "resnet34", "resnet50", "resnet101", "resnet152",
        "densenet121", "densenet169", "densenet201", "densenet161",
        "inception_v3", "googlenet", "shufflenet_v2_x0_5", "shufflenet_v2_x1_0",
        "mobilenet_v2", "mobilenet_v3_small", "mobilenet_v3_large",
        "resnext50_32x4d", "resnext101_32x8d", "wide_resnet50_2", "wide_resnet101_2",
        "mnasnet0_5", "mnasnet0_75", "mnasnet1_0", "mnasnet1_3",
        "efficientnet_b0", "efficientnet_b1", "efficientnet_b2", "efficientnet_b3",
        "efficientnet_b4", "efficientnet_b5", "efficientnet_b6", "efficientnet_b7",
        "regnet_y_400mf", "regnet_y_800mf", "regnet_y_1_6gf", "regnet_y_3_2gf",
        "vit_b_16", "vit_b_32", "vit_l_16", "vit_l_32", "vit_h_14",
    ]
    
    for model_name in available_models:
        print(f"- {model_name}")

# 加载并检查预训练模型的结构
def inspect_pretrained_model(model_name):
    print(f"\n=== 检查预训练模型: {model_name} ===")
    
    try:
        # 动态获取模型构造函数
        model_fn = getattr(models, model_name)
        # 加载预训练模型
        model = model_fn(pretrained=True)
        print(f"成功加载预训练模型: {model_name}")
        
        # 打印模型结构概览
        print("\n模型结构概览:")
        print(model)
        
        # 计算模型参数量
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"\n模型参数统计:")
        print(f"总参数量: {total_params:,}")
        print(f"可训练参数量: {trainable_params:,}")
        
        # 检查模型是否支持GPU
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)
        print(f"\n模型已移至设备: {device}")
        
        return model
    except Exception as e:
        print(f"加载模型失败: {e}")
        return None

# 主函数
def main():
    # 列出可用的预训练模型
    list_available_pretrained_models()
    
    # 选择几个代表性模型进行检查
    models_to_inspect = ["resnet18", "vgg16", "efficientnet_b0", "vit_b_16"]
    
    for model_name in models_to_inspect:
        inspect_pretrained_model(model_name)
        print("\n" + "="*50 + "\n")

if __name__ == "__main__":
    main()
```

## 5. 迁移学习与微调

### 理论知识点
迁移学习（Transfer Learning）是使用预训练模型的核心方法，它允许我们将从一个任务中学到的知识应用到另一个相关任务中。微调（Fine-tuning）是迁移学习的一种常见实现方式。

迁移学习的主要策略包括：

1. **特征提取**：
   - 固定预训练模型的大部分参数
   - 仅替换或训练最后几层（通常是分类层）
   - 适用于数据集较小或任务与预训练任务非常相似的情况

2. **部分微调**：
   - 固定预训练模型的前几层参数
   - 微调后面几层的参数
   - 适用于数据集中等大小的情况

3. **全微调**：
   - 微调所有层的参数，但通常设置较小的学习率
   - 适用于数据集较大或任务与预训练任务有一定差异的情况

### 实践示例：使用PyTorch进行迁移学习和微调

```python
# 使用PyTorch进行迁移学习和微调

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, random_split
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm

# 设置中文显示
plt.rcParams["font.family"] = ["SimHei", "WenQuanYi Micro Hei", "Heiti TC"]
plt.rcParams["axes.unicode_minus"] = False

# 数据准备
def prepare_data(data_dir='./data', batch_size=32):
    # 定义数据转换
    train_transform = transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    test_transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # 加载CIFAR-10数据集
    train_dataset = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=train_transform)
    test_dataset = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=test_transform)
    
    # 分割训练集和验证集
    train_size = int(0.8 * len(train_dataset))
    val_size = len(train_dataset) - train_size
    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])
    
    # 创建数据加载器
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
    
    return train_loader, val_loader, test_loader

# 创建微调模型
def create_finetuned_model(num_classes=10, freeze_base=True):
    # 加载预训练的ResNet18模型
    model = torchvision.models.resnet18(pretrained=True)
    
    # 是否冻结基础网络参数
    if freeze_base:
        for param in model.parameters():
            param.requires_grad = False
    
    # 替换最后的全连接层以适应新任务
    num_ftrs = model.fc.in_features
    model.fc = nn.Linear(num_ftrs, num_classes)
    
    return model

# 训练模型
def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device='cpu'):
    # 将模型移至指定设备
    model = model.to(device)
    
    # 记录训练和验证的损失和准确率
    train_losses = []
    val_losses = []
    train_accs = []
    val_accs = []
    
    # 开始训练
    for epoch in range(num_epochs):
        print(f'\nEpoch {epoch+1}/{num_epochs}')
        print('-' * 10)
        
        # 训练阶段
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        for inputs, labels in tqdm(train_loader, desc='Training'):
            inputs, labels = inputs.to(device), labels.to(device)
            
            # 梯度清零
            optimizer.zero_grad()
            
            # 前向传播
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            
            # 反向传播和优化
            loss.backward()
            optimizer.step()
            
            # 统计损失和准确率
            train_loss += loss.item() * inputs.size(0)
            _, predicted = torch.max(outputs, 1)
            train_total += labels.size(0)
            train_correct += (predicted == labels).sum().item()
        
        # 计算训练集的平均损失和准确率
        train_epoch_loss = train_loss / len(train_loader.dataset)
        train_epoch_acc = train_correct / train_total
        train_losses.append(train_epoch_loss)
        train_accs.append(train_epoch_acc)
        
        # 验证阶段
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for inputs, labels in tqdm(val_loader, desc='Validation'):
                inputs, labels = inputs.to(device), labels.to(device)
                
                # 前向传播
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                
                # 统计损失和准确率
                val_loss += loss.item() * inputs.size(0)
                _, predicted = torch.max(outputs, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()
        
        # 计算验证集的平均损失和准确率
        val_epoch_loss = val_loss / len(val_loader.dataset)
        val_epoch_acc = val_correct / val_total
        val_losses.append(val_epoch_loss)
        val_accs.append(val_epoch_acc)
        
        # 打印训练和验证结果
        print(f'Train Loss: {train_epoch_loss:.4f} Acc: {train_epoch_acc:.4f}')
        print(f'Validation Loss: {val_epoch_loss:.4f} Acc: {val_epoch_acc:.4f}')
    
    return model, train_losses, val_losses, train_accs, val_accs

# 评估模型
def evaluate_model(model, test_loader, device='cpu'):
    model = model.to(device)
    model.eval()
    
    test_correct = 0
    test_total = 0
    
    with torch.no_grad():
        for inputs, labels in tqdm(test_loader, desc='Testing'):
            inputs, labels = inputs.to(device), labels.to(device)
            
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            
            test_total += labels.size(0)
            test_correct += (predicted == labels).sum().item()
    
    test_acc = test_correct / test_total
    print(f'Test Accuracy: {test_acc:.4f}')
    
    return test_acc

# 可视化训练过程
def plot_training_results(train_losses, val_losses, train_accs, val_accs):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
    
    # 绘制损失曲线
    ax1.plot(range(1, len(train_losses) + 1), train_losses, 'o-', label='训练损失')
    ax1.plot(range(1, len(val_losses) + 1), val_losses, 's-', label='验证损失')
    ax1.set_title('训练和验证损失')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.grid(True)
    ax1.legend()
    
    # 绘制准确率曲线
    ax2.plot(range(1, len(train_accs) + 1), train_accs, 'o-', label='训练准确率')
    ax2.plot(range(1, len(val_accs) + 1), val_accs, 's-', label='验证准确率')
    ax2.set_title('训练和验证准确率')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy')
    ax2.grid(True)
    ax2.legend()
    
    plt.tight_layout()
    plt.show()

# 主函数
def main():
    # 设置设备
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f'使用设备: {device}')
    
    # 准备数据
    train_loader, val_loader, test_loader = prepare_data()
    
    # 定义类别名称（CIFAR-10）
    class_names = ['飞机', '汽车', '鸟类', '猫', '鹿', '狗', '青蛙', '马', '船', '卡车']
    print(f'类别数量: {len(class_names)}')
    print(f'类别名称: {class_names}')
    
    # 示例1：仅微调分类层（冻结基础网络）
    print('\n=== 示例1：仅微调分类层（冻结基础网络） ===')
    model1 = create_finetuned_model(num_classes=len(class_names), freeze_base=True)
    
    # 定义损失函数和优化器
    criterion = nn.CrossEntropyLoss()
    # 只优化分类层的参数
    optimizer1 = optim.Adam(model1.fc.parameters(), lr=0.001)
    
    # 训练模型
    model1, train_losses1, val_losses1, train_accs1, val_accs1 = train_model(
        model1, train_loader, val_loader, criterion, optimizer1, num_epochs=5, device=device
    )
    
    # 评估模型
    print('评估模型性能...')
    test_acc1 = evaluate_model(model1, test_loader, device=device)
    
    # 可视化训练结果
    plot_training_results(train_losses1, val_losses1, train_accs1, val_accs1)
    
    # 示例2：微调所有层（不冻结基础网络）
    print('\n=== 示例2：微调所有层（不冻结基础网络） ===')
    model2 = create_finetuned_model(num_classes=len(class_names), freeze_base=False)
    
    # 定义损失函数和优化器
    criterion = nn.CrossEntropyLoss()
    # 优化所有参数，但使用较小的学习率
    optimizer2 = optim.Adam(model2.parameters(), lr=0.0001)
    
    # 训练模型
    model2, train_losses2, val_losses2, train_accs2, val_accs2 = train_model(
        model2, train_loader, val_loader, criterion, optimizer2, num_epochs=5, device=device
    )
    
    # 评估模型
    print('评估模型性能...')
    test_acc2 = evaluate_model(model2, test_loader, device=device)
    
    # 可视化训练结果
    plot_training_results(train_losses2, val_losses2, train_accs2, val_accs2)
    
    # 比较两种微调策略的结果
    print('\n=== 两种微调策略的结果比较 ===')
    print(f'仅微调分类层: 测试准确率 = {test_acc1:.4f}')
    print(f'微调所有层: 测试准确率 = {test_acc2:.4f}')
    
    if test_acc1 > test_acc2:
        print('结论: 在当前任务中，仅微调分类层的效果更好')
    elif test_acc2 > test_acc1:
        print('结论: 在当前任务中，微调所有层的效果更好')
    else:
        print('结论: 两种微调策略的效果相当')

if __name__ == "__main__":
    main()
```

## 6. 使用Hugging Face Transformers库

### 理论知识点
Hugging Face Transformers库是一个强大的NLP工具库，它提供了大量预训练模型的实现和接口，使得使用预训练模型变得非常简单。该库的主要特点包括：

1. **丰富的预训练模型**：支持BERT、GPT-2、RoBERTa、T5等多种预训练模型
2. **统一的API接口**：提供一致的接口用于不同模型的加载、使用和微调
3. **便捷的分词器**：内置各种模型对应的分词器，处理文本输入
4. **支持多种下游任务**：分类、问答、翻译、摘要等
5. **支持模型共享和部署**：可以方便地分享和部署自己训练的模型

### 实践示例：使用Hugging Face Transformers进行文本分类

```python
# 使用Hugging Face Transformers进行文本分类

# 注意：运行此代码需要安装transformers库，可以使用命令`pip install transformers`进行安装

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset, load_metric
import numpy as np
import matplotlib.pyplot as plt

# 设置中文显示
plt.rcParams["font.family"] = ["SimHei", "WenQuanYi Micro Hei", "Heiti TC"]
plt.rcParams["axes.unicode_minus"] = False

# 加载数据集
def load_and_preprocess_data():
    # 加载情感分析数据集
    dataset = load_dataset("imdb")
    print(f"数据集大小: {len(dataset['train'])}")
    print(f"示例: {dataset['train'][0]}")
    
    return dataset

# 预处理函数
def preprocess_function(examples, tokenizer, max_length=128):
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=max_length)

# 评估函数
def compute_metrics(eval_pred):
    metric = load_metric("accuracy")
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

# 加载预训练模型和分词器
def load_pretrained_model(model_name="distilbert-base-uncased"):
    # 加载分词器
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # 加载预训练模型，指定分类任务和类别数量
    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)
    
    return tokenizer, model

# 微调预训练模型
def fine_tune_model(model, tokenizer, dataset):
    # 预处理数据集
    encoded_dataset = dataset.map(lambda examples: preprocess_function(examples, tokenizer), batched=True)
    
    # 定义训练参数
    training_args = TrainingArguments(
        output_dir="./results",          # 输出目录
        evaluation_strategy="epoch",     # 评估策略
        learning_rate=2e-5,              # 学习率
        per_device_train_batch_size=16,  # 训练批量大小
        per_device_eval_batch_size=16,   # 评估批量大小
        num_train_epochs=3,              # 训练轮数
        weight_decay=0.01,               # 权重衰减
        save_total_limit=1,              # 最多保存的检查点数量
    )
    
    # 创建Trainer实例
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=encoded_dataset["train"].shuffle(seed=42).select(range(5000)),  # 使用部分数据以加快训练
        eval_dataset=encoded_dataset["test"].shuffle(seed=42).select(range(1000)),    # 使用部分数据以加快评估
        compute_metrics=compute_metrics,
    )
    
    # 开始微调
    trainer.train()
    
    # 评估模型
    eval_results = trainer.evaluate()
    print(f"评估结果: {eval_results}")
    
    return trainer, eval_results

# 使用微调后的模型进行预测
def predict_with_model(model, tokenizer, text):
    # 设置模型为评估模式
    model.eval()
    
    # 预处理输入文本
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    
    # 进行预测
    with torch.no_grad():
        outputs = model(**inputs)
        
    # 获取预测结果
    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    
    # 返回预测类别和置信度
    class_names = ["负面", "正面"]
    confidence = torch.softmax(logits, dim=-1).max().item()
    
    return class_names[predictions.item()], confidence

# 主函数
def main():
    # 加载并预处理数据
    print("加载数据集...")
    dataset = load_and_preprocess_data()
    
    # 加载预训练模型和分词器
    print("加载预训练模型和分词器...")
    tokenizer, model = load_pretrained_model()
    
    # 微调模型
    print("微调预训练模型...")
    trainer, eval_results = fine_tune_model(model, tokenizer, dataset)
    
    # 使用微调后的模型进行预测
    print("\n使用微调后的模型进行预测:")
    
    # 测试一些示例文本
    test_texts = [
        "这部电影真是太棒了，演员的表演非常出色，剧情也很吸引人。",
        "我从未看过如此糟糕的电影，简直是在浪费时间。",
        "这部电影还行，没有特别出彩的地方，但也不难看。"
    ]
    
    for text in test_texts:
        sentiment, confidence = predict_with_model(model, tokenizer, text)
        print(f"文本: {text}")
        print(f"情感: {sentiment}, 置信度: {confidence:.4f}")
        print("---")

if __name__ == "__main__":
    main()
```

## 7. 预训练模型的部署和优化

### 理论知识点
部署预训练模型时，我们通常需要考虑模型的大小、推理速度和硬件资源限制等问题。以下是一些常用的模型部署和优化技巧：

1. **模型量化**：
   - 将32位浮点数参数转换为16位浮点数或8位整数
   - 显著减小模型大小，加快推理速度
   - 常用方法包括动态量化、静态量化和量化感知训练

2. **模型剪枝**：
   - 移除模型中不重要的连接或神经元
   - 减小模型大小，同时尽量保持性能
   - 可以通过权重阈值、稀疏训练等方法实现

3. **知识蒸馏**：
   - 将大型模型（教师模型）的知识转移到小型模型（学生模型）
   - 学生模型通常比教师模型小得多，但性能接近
   - 适用于资源受限的部署环境

4. **模型导出和优化框架**：
   - ONNX (Open Neural Network Exchange)：神经网络模型的开放格式
   - TorchScript：PyTorch模型的序列化格式
   - TensorRT：NVIDIA的高性能深度学习推理SDK
   - TFLite：TensorFlow的轻量级推理框架

### 实践示例：模型量化与优化

```python
# 简单示例：模型量化与优化

import torch
import torchvision.models as models
import time
import os

# 模型量化与优化示例
def model_quantization_example():
    print("=== 模型量化与优化示例 ===")
    
    # 1. 加载预训练模型
    print("1. 加载预训练模型...")
    model = models.resnet18(pretrained=True)
    model.eval()  # 设置为评估模式
    
    # 2. 创建示例输入
    print("2. 创建示例输入...")
    example_input = torch.randn(1, 3, 224, 224)  # 模拟一张RGB图像
    
    # 3. 测量原始模型的性能
    print("3. 测量原始模型的性能...")
    # 测量推理时间
    start_time = time.time()
    with torch.no_grad():
        for _ in range(10):
            output = model(example_input)
    end_time = time.time()
    avg_inference_time = (end_time - start_time) / 10
    print(f"原始模型平均推理时间: {avg_inference_time:.4f}秒")
    
    # 保存原始模型并测量大小
    torch.save(model, "original_model.pth")
    original_size = os.path.getsize("original_model.pth") / (1024 * 1024)  # MB
    print(f"原始模型大小: {original_size:.2f} MB")
    
    # 4. 应用动态量化
    print("\n4. 应用动态量化...")
    quantized_model = torch.quantization.quantize_dynamic(
        model,  # 要量化的模型
        {torch.nn.Linear},  # 指定要量化的层类型
        dtype=torch.qint8  # 量化类型
    )
    
    # 测量量化模型的性能
    start_time = time.time()
    with torch.no_grad():
        for _ in range(10):
            output = quantized_model(example_input)
    end_time = time.time()
    quantized_inference_time = (end_time - start_time) / 10
    print(f"量化模型平均推理时间: {quantized_inference_time:.4f}秒")
    
    # 保存量化模型并测量大小
    torch.save(quantized_model, "quantized_model.pth")
    quantized_size = os.path.getsize("quantized_model.pth") / (1024 * 1024)  # MB
    print(f"量化模型大小: {quantized_size:.2f} MB")
    
    # 5. 比较结果
    print("\n5. 比较结果:")
    print(f"模型大小减小比例: {100 * (1 - quantized_size / original_size):.2f}%")
    if quantized_inference_time < avg_inference_time:
        speedup = avg_inference_time / quantized_inference_time
        print(f"推理速度提升: {speedup:.2f}倍")
    else:
        slowdown = quantized_inference_time / avg_inference_time
        print(f"推理速度下降: {slowdown:.2f}倍")
    
    # 6. 导出为TorchScript（优化部署）
    print("\n6. 导出为TorchScript...")
    # 跟踪模型
    traced_model = torch.jit.trace(model, example_input)
    # 保存TorchScript模型
    torch.jit.save(traced_model, "traced_model.pt")
    traced_size = os.path.getsize("traced_model.pt") / (1024 * 1024)  # MB
    print(f"TorchScript模型大小: {traced_size:.2f} MB")
    
    # 7. 清理文件
    print("\n7. 清理临时文件...")
    for file in ["original_model.pth", "quantized_model.pth", "traced_model.pt"]:
        if os.path.exists(file):
            os.remove(file)
    
    print("\n=== 总结 ===")
    print("- 模型量化可以显著减小模型大小")
    print("- 在某些硬件上，量化模型还可以提高推理速度")
    print("- TorchScript等优化技术可以进一步优化模型部署")
    print("- 实际应用中需要根据具体硬件和任务选择合适的优化策略")

# 主函数
def main():
    model_quantization_example()

if __name__ == "__main__":
    main()
```

## 8. 总结与下一步

通过本教程，你学习了预训练模型的基础知识，包括：
- 预训练模型的概念和优势
- 常见的预训练模型及其应用场景
- 迁移学习和微调的基本原理和方法
- 使用PyTorch和Hugging Face Transformers库加载和微调预训练模型
- 模型部署和优化的基本技巧

这些知识将帮助你快速上手使用预训练模型，并将其应用到自己的项目中。

**下一步**：请继续学习**No006-计算机视觉基础.md**，了解计算机视觉的基本概念、技术和应用，并学习如何使用预训练模型解决计算机视觉任务。