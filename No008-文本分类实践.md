# No008-文本分类实践：使用预训练模型解决实际问题

## 1. 文本分类概述

文本分类是自然语言处理中的一项基础任务，它的目标是将文本划分到预定义的类别中。常见的文本分类任务包括情感分析、主题分类、垃圾邮件检测等。在本教程中，我们将学习如何使用预训练模型来解决文本分类问题。

## 2. 准备工作

### 理论知识点
在开始文本分类实践之前，我们需要准备必要的库、数据和环境。

### 实践示例：安装必要的库和设置环境

```python
# 安装必要的库
# 可以在终端中执行以下命令：
# pip install transformers datasets torch pandas numpy scikit-learn matplotlib

# 也可以在Jupyter Notebook或Python脚本中使用以下代码安装：
import sys
!{sys.executable} -m pip install transformers datasets torch pandas numpy scikit-learn matplotlib

# 导入必要的库
import torch
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset, DatasetDict

# 设置中文显示
plt.rcParams["font.family"] = ["SimHei", "WenQuanYi Micro Hei", "Heiti TC"]
plt.rcParams["axes.unicode_minus"] = False

# 检查是否有GPU可用
print(f"CUDA是否可用: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU设备: {torch.cuda.get_device_name(0)}")
    device = torch.device("cuda")
else:
    print("使用CPU")
    device = torch.device("cpu")
```

## 3. 加载和探索数据集

### 理论知识点
文本分类任务的第一步是准备和探索数据集。一个好的数据集应该包含足够的样本，并且类别分布合理。

### 实践示例：加载和探索情感分析数据集

```python
# 加载和探索情感分析数据集

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datasets import load_dataset

# 加载数据集（这里使用Hugging Face的datasets库加载情感分析数据集）
print("加载IMDB电影评论情感分析数据集...")
dataset = load_dataset("imdb")

# 查看数据集结构
print("\n数据集结构:")
print(dataset)

# 查看训练集、验证集和测试集的大小
print("\n数据集大小:")
print(f"训练集: {len(dataset['train'])}")
print(f"测试集: {len(dataset['test'])}")

# 查看示例数据
print("\n示例数据:")
for i in range(3):
    print(f"\n示例 {i+1}:")
    print(f"文本: {dataset['train'][i]['text'][:200]}...")
    print(f"标签: {dataset['train'][i]['label']} ({'正面' if dataset['train'][i]['label'] == 1 else '负面'})")

# 分析标签分布
labels = [example['label'] for example in dataset['train']]
label_counts = pd.Series(labels).value_counts()

print("\n标签分布:")
print(label_counts)

# 可视化标签分布
plt.figure(figsize=(8, 5))
label_counts.plot(kind='bar', rot=0)
plt.title('IMDB数据集标签分布')
plt.xlabel('标签')
plt.ylabel('样本数量')
plt.xticks([0, 1], ['负面 (0)', '正面 (1)'])
plt.show()

# 分析文本长度分布
text_lengths = [len(example['text'].split()) for example in dataset['train']]

print("\n文本长度统计:")
print(f"最短文本长度: {min(text_lengths)} 词")
print(f"最长文本长度: {max(text_lengths)} 词")
print(f"平均文本长度: {np.mean(text_lengths):.2f} 词")
print(f"中位数文本长度: {np.median(text_lengths)} 词")

# 可视化文本长度分布
plt.figure(figsize=(10, 6))
plt.hist(text_lengths, bins=50, range=(0, 1000), alpha=0.7)
plt.title('IMDB数据集文本长度分布')
plt.xlabel('文本长度 (词数)')
plt.ylabel('样本数量')
plt.grid(True, alpha=0.3)
plt.show()

# 注意：对于大规模数据集，我们可能需要创建一个较小的子集进行快速实验
print("\n提示：对于大规模数据集，可以创建一个子集进行快速实验。")
print("例如，使用数据集的前1000个样本进行训练和验证：")
print("small_train_dataset = dataset['train'].select(range(1000))")
print("small_test_dataset = dataset['test'].select(range(1000))")
```

## 4. 数据预处理

### 理论知识点
在使用预训练模型之前，我们需要对数据进行预处理，包括分词、添加特殊标记和填充等。

### 实践示例：预处理文本数据

```python
# 预处理文本数据

from transformers import AutoTokenizer
import torch

# 选择一个预训练模型
model_name = "distilbert-base-uncased"

# 加载分词器
print(f"加载分词器: {model_name}...")
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 定义预处理函数
def preprocess_function(examples):
    return tokenizer(
        examples["text"],
        padding="max_length",  # 填充到最大长度
        truncation=True,       # 截断过长的文本
        max_length=256,        # 最大长度，可以根据需要调整
        return_tensors="pt"    # 返回PyTorch张量
    )

# 测试预处理函数
print("\n测试预处理函数:")
sample_text = "This is a sample text for preprocessing."
sample_inputs = tokenizer(sample_text, padding="max_length", truncation=True, max_length=10, return_tensors="pt")
print(f"原始文本: {sample_text}")
print(f"分词结果: {tokenizer.tokenize(sample_text)}")
print(f"输入ID: {sample_inputs['input_ids']}")
print(f"注意力掩码: {sample_inputs['attention_mask']}")

# 对数据集应用预处理函数
print("\n对数据集应用预处理函数...")

# 注意：为了加快处理速度，我们只使用一小部分数据集
small_dataset = {
    "train": dataset["train"].select(range(1000)),  # 仅使用前1000个训练样本
    "test": dataset["test"].select(range(500))     # 仅使用前500个测试样本
}

# 应用预处理函数
print("预处理训练集...")
encoded_train_dataset = small_dataset["train"].map(preprocess_function, batched=True)
print("预处理测试集...")
encoded_test_dataset = small_dataset["test"].map(preprocess_function, batched=True)

# 设置数据集格式为PyTorch
encoded_train_dataset.set_format("torch", columns=["input_ids", "attention_mask", "label"])
encoded_test_dataset.set_format("torch", columns=["input_ids", "attention_mask", "label"])

# 查看处理后的数据集
print("\n处理后的数据集示例:")
print(f"输入ID形状: {encoded_train_dataset[0]['input_ids'].shape}")
print(f"注意力掩码形状: {encoded_train_dataset[0]['attention_mask'].shape}")
print(f"标签: {encoded_train_dataset[0]['label']}")
```

## 5. 加载预训练模型并微调

### 理论知识点
微调（Fine-tuning）是指在预训练模型的基础上，使用特定任务的数据集进行进一步训练，使模型适应特定任务。

### 实践示例：加载预训练模型并进行微调

```python
# 加载预训练模型并进行微调

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer
import torch

# 加载预训练模型用于序列分类
model_name = "distilbert-base-uncased"
print(f"加载预训练模型: {model_name}...")
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=2,  # 二分类任务
    id2label={0: "负面", 1: "正面"},
    label2id={"负面": 0, "正面": 1}
)

# 将模型移至GPU（如果可用）
model.to(device)

# 定义训练参数
print("\n设置训练参数...")
training_args = TrainingArguments(
    output_dir="./results",  # 输出目录
    num_train_epochs=3,      # 训练轮数
    per_device_train_batch_size=16,  # 训练批次大小
    per_device_eval_batch_size=16,   # 评估批次大小
    warmup_steps=500,        # 预热步数
    weight_decay=0.01,       # 权重衰减
    logging_dir="./logs",    # 日志目录
    logging_steps=10,        # 每10步记录一次日志
    evaluation_strategy="epoch",  # 每个epoch评估一次
    save_strategy="epoch",   # 每个epoch保存一次模型
    load_best_model_at_end=True,  # 训练结束后加载最佳模型
    fp16=torch.cuda.is_available(),  # 如果有GPU，使用混合精度训练
    gradient_accumulation_steps=1,  # 梯度累积步数
    learning_rate=2e-5,      # 学习率
)

# 创建Trainer实例
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_train_dataset,
    eval_dataset=encoded_test_dataset,
    tokenizer=tokenizer,
)

# 开始训练
print("\n开始训练模型...")
training_results = trainer.train()

# 查看训练结果
print("\n训练结果:")
print(training_results)

# 保存最佳模型
print("\n保存最佳模型...")
trainer.save_model("./best_model")
tokenizer.save_pretrained("./best_model")
```

## 6. 模型评估

### 理论知识点
训练完成后，我们需要评估模型的性能，以了解它在未见过的数据上的表现。常见的评估指标包括准确率、精确率、召回率和F1分数等。

### 实践示例：评估模型性能

```python
# 评估模型性能

from transformers import Trainer
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

# 使用Trainer进行评估
print("评估模型性能...")
eval_results = trainer.evaluate()

print("\n评估结果:")
for key, value in eval_results.items():
    print(f"{key}: {value:.4f}")

# 获取预测结果
def get_predictions(trainer, dataset):
    predictions = trainer.predict(dataset)
    pred_labels = np.argmax(predictions.predictions, axis=1)
    return pred_labels, predictions.label_ids

# 获取测试集的预测结果
print("\n获取测试集的预测结果...")
pred_labels, true_labels = get_predictions(trainer, encoded_test_dataset)

# 计算准确率
accuracy = (pred_labels == true_labels).mean()
print(f"测试集准确率: {accuracy:.4f}")

# 生成分类报告
print("\n分类报告:")
print(classification_report(true_labels, pred_labels, target_names=["负面", "正面"], digits=4))

# 生成混淆矩阵
cm = confusion_matrix(true_labels, pred_labels)

# 可视化混淆矩阵
plt.figure(figsize=(8, 6))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["负面", "正面"])
disp.plot(cmap=plt.cm.Blues)
plt.title('测试集混淆矩阵')
plt.show()

# 分析错误预测
print("\n分析错误预测...")
incorrect_indices = np.where(pred_labels != true_labels)[0]

if len(incorrect_indices) > 0:
    # 显示一些错误预测的示例
    print(f"共有 {len(incorrect_indices)} 个错误预测，显示前3个示例:")
    
    for i in range(min(3, len(incorrect_indices))):
        idx = incorrect_indices[i]
        text = small_dataset["test"][idx]["text"]
        true_label = true_labels[idx]
        pred_label = pred_labels[idx]
        
        print(f"\n错误示例 {i+1}:")
        print(f"真实标签: {true_label} ({'正面' if true_label == 1 else '负面'})")
        print(f"预测标签: {pred_label} ({'正面' if pred_label == 1 else '负面'})")
        print(f"文本: {text[:300]}...")
else:
    print("没有错误预测！")
```

## 7. 使用模型进行预测

### 理论知识点
训练好的模型可以用于对新的、未标记的文本进行预测。

### 实践示例：使用训练好的模型进行预测

```python
# 使用训练好的模型进行预测

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# 加载训练好的模型和分词器
model_path = "./best_model"
print(f"加载训练好的模型和分词器: {model_path}...")
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)

# 将模型移至GPU（如果可用）
model.to(device)
model.eval()  # 设置为评估模式

# 定义预测函数
def predict_sentiment(text, model, tokenizer, device):
    # 预处理文本
    inputs = tokenizer(
        text,
        padding="max_length",
        truncation=True,
        max_length=256,
        return_tensors="pt"
    ).to(device)
    
    # 进行预测
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probabilities = torch.softmax(logits, dim=1)
        predicted_label = torch.argmax(probabilities, dim=1).item()
    
    # 获取标签和概率
    label = model.config.id2label[predicted_label]
    confidence = probabilities[0][predicted_label].item()
    
    return label, confidence

# 测试自定义文本
custom_texts = [
    "This movie was absolutely fantastic! I loved every minute of it.",
    "I would not recommend this film to anyone. It was boring and predictable.",
    "The acting was decent, but the plot was a bit confusing.",
    "这是一部非常精彩的电影，我非常喜欢！",  # 测试中文文本（注意：我们的模型是英文模型，可能对中文表现不佳）
]

print("\n对自定义文本进行情感分析:")
for text in custom_texts:
    label, confidence = predict_sentiment(text, model, tokenizer, device)
    print(f"文本: {text}")
    print(f"情感: {label}, 置信度: {confidence:.4f}")
    print("---")

# 提示：对于中文文本分类，应该使用专门针对中文训练的模型
print("\n提示：对于中文文本分类，建议使用专门针对中文训练的模型，例如：")
print("- hfl/chinese-roberta-wwm-ext")
print("- hfl/chinese-bert-wwm-ext")
print("- IDEA-CCNL/Erlangshen-Roberta-330M-Sentiment")
```

## 8. 完整项目实践

### 理论知识点
现在，让我们将前面学到的知识整合起来，构建一个完整的文本分类项目，包括数据准备、模型定义、训练、评估和部署。

### 实践示例：完整的文本分类项目

```python
# 完整的文本分类项目：中文情感分析

import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from datasets import Dataset, DatasetDict
import os

# 设置中文显示
plt.rcParams["font.family"] = ["SimHei", "WenQuanYi Micro Hei", "Heiti TC"]
plt.rcParams["axes.unicode_minus"] = False

# 检查是否有GPU可用
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"使用设备: {device}")

# 1. 准备数据
# 这里我们使用一个简单的中文情感分析数据集示例
# 实际应用中，你可以替换为自己的数据集

def prepare_dataset():
    # 创建一个简单的中文情感分析数据集
    # 实际应用中，你应该从文件或数据库中加载数据
    data = {
        "text": [
            "这家餐厅的食物非常美味，服务也很周到。",
            "电影情节很精彩，演员表演也很出色。",
            "这个产品质量很差，完全不值得购买。",
            "今天的天气真好，心情也跟着变好了。",
            "这家酒店环境很差，价格还很贵。",
            "这本书写得非常好，我一口气就读完了。",
            "这个手机电池续航很差，一天要充好几次电。",
            "这家咖啡店的咖啡很香，环境也很安静。",
            "这个APP界面设计很糟糕，使用起来很不方便。",
            "这次旅行很愉快，看到了很多美丽的风景。",
            # 更多示例...
        ],
        "label": [1, 1, 0, 1, 0, 1, 0, 1, 0, 1]  # 1表示正面，0表示负面
    }
    
    # 创建DataFrame
    df = pd.DataFrame(data)
    
    # 分割训练集和测试集
    train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)
    
    # 转换为Hugging Face的Dataset格式
    train_dataset = Dataset.from_pandas(train_df)
    test_dataset = Dataset.from_pandas(test_df)
    
    return DatasetDict({"train": train_dataset, "test": test_dataset})

# 2. 加载数据集
dataset = prepare_dataset()
print("数据集大小:")
print(f"训练集: {len(dataset['train'])}")
print(f"测试集: {len(dataset['test'])}")

# 3. 选择预训练模型
model_name = "hfl/chinese-roberta-wwm-ext"  # 中文RoBERTa模型

# 4. 加载分词器和模型
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=2,
    id2label={0: "负面", 1: "正面"},
    label2id={"负面": 0, "正面": 1}
)
model.to(device)

# 5. 预处理数据
def preprocess_function(examples):
    return tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=128,
        return_tensors="pt"
    )

encoded_dataset = dataset.map(preprocess_function, batched=True)
encoded_dataset.set_format("torch", columns=["input_ids", "attention_mask", "label"])

# 6. 设置训练参数
training_args = TrainingArguments(
    output_dir="./chinese_sentiment_results",
    num_train_epochs=5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    warmup_steps=10,
    weight_decay=0.01,
    logging_dir="./chinese_sentiment_logs",
    logging_steps=5,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    fp16=torch.cuda.is_available(),
    learning_rate=2e-5,
)

# 7. 创建Trainer并训练
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset["test"],
    tokenizer=tokenizer,
)

# 开始训练
trainer.train()

# 8. 评估模型
eval_results = trainer.evaluate()
print("评估结果:")
print(eval_results)

# 9. 生成分类报告和混淆矩阵
def get_predictions(trainer, dataset):
    predictions = trainer.predict(dataset)
    pred_labels = np.argmax(predictions.predictions, axis=1)
    return pred_labels, predictions.label_ids

pred_labels, true_labels = get_predictions(trainer, encoded_dataset["test"])

print("分类报告:")
print(classification_report(true_labels, pred_labels, target_names=["负面", "正面"], digits=4))

# 可视化混淆矩阵
cm = confusion_matrix(true_labels, pred_labels)
plt.figure(figsize=(8, 6))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["负面", "正面"])
disp.plot(cmap=plt.cm.Blues)
plt.title('混淆矩阵')
plt.savefig('chinese_sentiment_confusion_matrix.png')
plt.show()

# 10. 保存模型
trainer.save_model("./chinese_sentiment_model")
tokenizer.save_pretrained("./chinese_sentiment_model")
print("模型已保存到 ./chinese_sentiment_model")

# 11. 使用模型进行预测

def predict_chinese_sentiment(text):
    # 加载训练好的模型
    loaded_model = AutoModelForSequenceClassification.from_pretrained("./chinese_sentiment_model")
    loaded_tokenizer = AutoTokenizer.from_pretrained("./chinese_sentiment_model")
    loaded_model.to(device)
    loaded_model.eval()
    
    # 预处理文本
    inputs = loaded_tokenizer(
        text,
        padding="max_length",
        truncation=True,
        max_length=128,
        return_tensors="pt"
    ).to(device)
    
    # 进行预测
    with torch.no_grad():
        outputs = loaded_model(**inputs)
        logits = outputs.logits
        probabilities = torch.softmax(logits, dim=1)
        predicted_label = torch.argmax(probabilities, dim=1).item()
    
    # 获取标签和概率
    label = loaded_model.config.id2label[predicted_label]
    confidence = probabilities[0][predicted_label].item()
    
    return label, confidence

# 测试预测函数
new_texts = [
    "这家餐厅的菜真的很好吃，服务也很热情。",
    "这个产品质量太差了，用了两天就坏了。",
    "今天的会议很无聊，浪费了很多时间。",
    "这部电影很精彩，我想再看一遍。"
]

print("\n对新文本进行情感分析:")
for text in new_texts:
    label, confidence = predict_chinese_sentiment(text)
    print(f"文本: {text}")
    print(f"情感: {label}, 置信度: {confidence:.4f}")
    print("---")
```

## 9. 常见问题和解决方案

### 问题1：训练过程中显存不足
- **症状**：训练过程中出现CUDA out of memory错误
- **解决方案**：减小批次大小，使用梯度累积，或使用更小的模型

```python
# 减小批次大小和使用梯度累积的示例
training_args = TrainingArguments(
    per_device_train_batch_size=8,  # 减小批次大小
    gradient_accumulation_steps=2,  # 梯度累积步数，相当于实际批次大小为16
    # 其他参数保持不变
)

# 使用更小的模型
model_name = "distilbert-base-uncased"  # 比bert-base小约40%
# 或者使用量化技术
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=2,
    load_in_8bit=True,  # 使用8位量化
    device_map="auto"
)
```

### 问题2：模型过拟合
- **症状**：训练损失不断下降，但验证损失不再下降或开始上升
- **解决方案**：增加正则化，使用Dropout，或收集更多数据

```python
# 增加正则化的示例
training_args = TrainingArguments(
    weight_decay=0.01,  # 增加权重衰减
    # 其他参数保持不变
)

# 使用早期停止
training_args = TrainingArguments(
    evaluation_strategy="steps",
    eval_steps=500,
    load_best_model_at_end=True,
    # 其他参数保持不变
)

# 数据增强（简单示例）
def augment_text(text):
    # 在实际应用中，你可以使用更复杂的数据增强技术
    import random
    augmented_texts = []
    
    # 同义词替换（简单版本）
    synonyms = {
        "好": ["不错", "优秀", "出色"],
        "坏": ["糟糕", "差劲", "恶劣"]
        # 可以添加更多同义词
    }
    
    words = text.split()
    new_words = []
    
    for word in words:
        if word in synonyms and random.random() < 0.3:  # 30%的概率替换同义词
            new_words.append(random.choice(synonyms[word]))
        else:
            new_words.append(word)
    
    augmented_texts.append(" ".join(new_words))
    
    return augmented_texts
```

### 问题3：模型在新数据上表现不佳
- **症状**：模型在测试集上表现良好，但在实际应用中表现不佳
- **解决方案**：确保训练数据和实际数据分布一致，进行领域适应，或收集更多实际数据

```python
# 领域适应的简单示例
# 在目标领域的小数据集上进行微调

def domain_adaptation(base_model_path, target_domain_dataset):
    # 加载基础模型
    model = AutoModelForSequenceClassification.from_pretrained(base_model_path)
    tokenizer = AutoTokenizer.from_pretrained(base_model_path)
    
    # 预处理目标领域数据
    encoded_target_dataset = target_domain_dataset.map(preprocess_function, batched=True)
    
    # 设置较小的学习率进行微调
    training_args = TrainingArguments(
        output_dir="./domain_adapted_model",
        num_train_epochs=2,  # 少量的训练轮数
        per_device_train_batch_size=4,
        learning_rate=1e-6,  # 非常小的学习率
        # 其他参数保持不变
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=encoded_target_dataset,
        tokenizer=tokenizer,
    )
    
    # 进行微调
    trainer.train()
    
    # 保存适应后的模型
    trainer.save_model("./domain_adapted_model")
    
    return model, tokenizer
```

## 10. 下一步学习建议

1. **探索更多文本分类模型**：如RoBERTa、ALBERT、ELECTRA等
2. **学习高级微调技术**：如参数高效微调（PEFT）、适配器（Adapter）等
3. **了解模型解释性**：学习如何解释模型的预测结果
4. **尝试多标签分类**：学习如何处理一个文本属于多个类别的情况
5. **探索零样本和少样本学习**：学习如何在标签数据有限的情况下进行分类

通过本教程，你应该已经掌握了使用预训练模型进行文本分类的基本技能。在下一章节中，我们将介绍知识库的概念和构建方法，为构建个人知识库助手奠定基础。