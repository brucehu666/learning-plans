# No013-综合项目：个人知识库助手

## 1. 项目概述与架构设计

### 1.1 项目概述
在本综合项目中，我们将整合之前学习的所有知识，创建一个完整的个人知识库助手应用。这个应用将能够：

1. 从多种格式的文档中提取知识（如PDF、Word、Markdown等）
2. 构建高效的向量知识库进行知识存储和检索
3. 利用本地部署的大语言模型进行智能问答
4. 通过RAG（检索增强生成）技术提供准确的回答
5. 提供简单直观的用户界面进行交互

### 1.2 系统架构
我们将采用模块化设计，构建一个完整的个人知识库助手系统。系统架构如下图所示：

```
+-------------------+    +-------------------+    +-------------------+
|   文档处理模块    | -> |   向量知识库模块   | <- |   检索模块       |
+-------------------+    +-------------------+    +-------------------+
                                                     |
                                                     v
+-------------------+    +-------------------+    +-------------------+
|   用户界面模块    | <- |   RAG协调模块     | <- |   LLM推理模块    |
+-------------------+    +-------------------+    +-------------------+
```

核心模块说明：
- **文档处理模块**：负责加载、解析和分割各种格式的文档
- **向量知识库模块**：使用FAISS构建高效的向量存储
- **LLM推理模块**：管理本地大语言模型的加载和推理
- **检索模块**：根据用户查询从知识库中检索相关信息
- **RAG协调模块**：协调检索和生成过程，实现RAG功能
- **用户界面模块**：提供交互界面，接收用户输入并展示结果

### 1.3 技术栈选择
针对Win11系统和RTX 3060 12G显卡环境，我们选择以下技术栈：

1. **基础框架**：Python 3.9+, PyTorch
2. **文档处理**：LangChain, pypdf, python-docx, markdown
3. **向量存储**：FAISS
4. **预训练模型**：Hugging Face Transformers, GPTQ量化模型
5. **本地LLM**：llama.cpp, vLLM
6. **用户界面**：Gradio
7. **工具库**：sentence-transformers, NumPy, Pandas

## 2. 环境准备与依赖安装

### 2.1 安装必要的软件和库

```python
# 在命令行中执行以下命令安装必要的依赖
# 基础依赖
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers accelerate sentencepiece

# 文档处理依赖
pip install langchain pypdf python-docx markdown

# 向量存储依赖
pip install faiss-cpu  # 对于GPU支持，可以安装faiss-gpu

# 嵌入模型依赖
pip install sentence-transformers

# 界面依赖
s:

# 本地LLM依赖
s:
```

### 2.2 项目目录结构
我们将按照以下目录结构组织项目文件：

```
personal_knowledge_assistant/
├── config/                # 配置文件
│   └── config.json        # 系统配置
├── data/                  # 数据存储
│   ├── documents/         # 原始文档
│   ├── processed/         # 处理后的文档
│   └── vector_db/         # 向量数据库
├── models/                # 模型存储
│   ├── llm/               # 大语言模型
│   └── embedding/         # 嵌入模型
├── src/                   # 源代码
│   ├── document_processor.py  # 文档处理模块
│   ├── vector_store.py        # 向量存储模块
│   ├── llm_engine.py          # LLM推理引擎
│   ├── retrieval.py           # 检索模块
│   ├── rag_coordinator.py     # RAG协调器
│   └── ui.py                  # 用户界面
├── main.py                # 主程序入口
└── requirements.txt       # 依赖列表
```

### 2.3 创建配置文件

```python
import json
import os

# 创建项目目录
def create_project_structure(base_dir):
    # 创建目录结构
    directories = [
        os.path.join(base_dir, 'config'),
        os.path.join(base_dir, 'data', 'documents'),
        os.path.join(base_dir, 'data', 'processed'),
        os.path.join(base_dir, 'data', 'vector_db'),
        os.path.join(base_dir, 'models', 'llm'),
        os.path.join(base_dir, 'models', 'embedding'),
        os.path.join(base_dir, 'src')
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
    
    print(f"项目目录结构已创建在: {base_dir}")

# 创建配置文件
def create_config_file(config_dir):
    config = {
        "embedding_model": {
            "name": "all-MiniLM-L6-v2",  # 轻量级嵌入模型
            "cache_dir": "models/embedding",
            "device": "cuda"  # 对于RTX 3060，使用CUDA
        },
        "llm": {
            "model_name": "TheBloke/Llama-2-7B-Chat-GPTQ",  # 适合RTX 3060的量化模型
            "cache_dir": "models/llm",
            "max_new_tokens": 512,
            "temperature": 0.7,
            "device_map": "auto"
        },
        "vector_store": {
            "type": "faiss",
            "path": "data/vector_db/faiss_index",
            "embedding_dim": 384,  # 对应all-MiniLM-L6-v2的嵌入维度
            "top_k": 5  # 检索时返回的文档数量
        },
        "document_processor": {
            "chunk_size": 500,
            "chunk_overlap": 50,
            "supported_formats": ["pdf", "docx", "md", "txt"]
        },
        "ui": {
            "theme": "default",
            "max_history": 20
        }
    }
    
    config_path = os.path.join(config_dir, "config.json")
    with open(config_path, "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    print(f"配置文件已创建在: {config_path}")

if __name__ == "__main__":
    # 设置基础目录
    base_dir = "personal_knowledge_assistant"
    
    # 创建项目结构
    create_project_structure(base_dir)
    
    # 创建配置文件
    create_config_file(os.path.join(base_dir, "config"))
```

## 3. 核心模块实现

### 3.1 文档处理模块
文档处理模块负责加载、解析和分割各种格式的文档。

```python
# src/document_processor.py
import os
import json
from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader, UnstructuredMarkdownLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

class DocumentProcessor:
    """文档处理模块，负责加载和处理各种格式的文档"""
    def __init__(self, config):
        self.config = config
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config["document_processor"]["chunk_size"],
            chunk_overlap=config["document_processor"]["chunk_overlap"],
            separators=["\n\n", "\n", " ", ""]
        )
        
    def load_document(self, file_path):
        """加载指定路径的文档"""
        _, ext = os.path.splitext(file_path)
        ext = ext.lower()[1:]  # 获取文件扩展名（不含点）
        
        if ext not in self.config["document_processor"]["supported_formats"]:
            raise ValueError(f"不支持的文件格式: {ext}")
        
        try:
            if ext == "pdf":
                loader = PyPDFLoader(file_path)
            elif ext == "docx":
                loader = Docx2txtLoader(file_path)
            elif ext == "md":
                loader = UnstructuredMarkdownLoader(file_path)
            elif ext == "txt":
                loader = TextLoader(file_path, encoding="utf-8")
            
            documents = loader.load()
            return documents
        except Exception as e:
            raise Exception(f"加载文档失败: {str(e)}")
    
    def process_document(self, file_path):
        """处理文档，包括加载和分割"""
        # 加载文档
        documents = self.load_document(file_path)
        
        # 分割文档
        chunks = self.text_splitter.split_documents(documents)
        
        # 为每个chunk添加元数据
        for i, chunk in enumerate(chunks):
            chunk.metadata["chunk_id"] = i
            chunk.metadata["file_name"] = os.path.basename(file_path)
        
        return chunks
    
    def batch_process_documents(self, directory_path):
        """批量处理目录中的所有文档"""
        all_chunks = []
        
        # 遍历目录中的所有文件
        for root, _, files in os.walk(directory_path):
            for file in files:
                _, ext = os.path.splitext(file)
                ext = ext.lower()[1:]
                
                if ext in self.config["document_processor"]["supported_formats"]:
                    file_path = os.path.join(root, file)
                    try:
                        chunks = self.process_document(file_path)
                        all_chunks.extend(chunks)
                        print(f"已处理文档: {file}")
                    except Exception as e:
                        print(f"处理文档 {file} 失败: {str(e)}")
        
        return all_chunks
    
    def save_processed_chunks(self, chunks, output_dir):
        """保存处理后的文档块"""
        os.makedirs(output_dir, exist_ok=True)
        
        for i, chunk in enumerate(chunks):
            chunk_data = {
                "id": i,
                "content": chunk.page_content,
                "metadata": chunk.metadata
            }
            
            output_path = os.path.join(output_dir, f"chunk_{i}.json")
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(chunk_data, f, ensure_ascii=False, indent=2)
        
        print(f"已保存 {len(chunks)} 个文档块到 {output_dir}")

# 测试文档处理模块
if __name__ == "__main__":
    # 加载配置
    with open("../config/config.json", "r", encoding="utf-8") as f:
        config = json.load(f)
    
    # 创建文档处理器
    processor = DocumentProcessor(config)
    
    # 测试单个文档处理
    test_doc_path = "../data/documents/sample.pdf"  # 替换为实际路径
    if os.path.exists(test_doc_path):
        chunks = processor.process_document(test_doc_path)
        print(f"处理完成，生成了 {len(chunks)} 个文档块")
        print(f"第一个文档块内容预览: {chunks[0].page_content[:100]}...")
    
    # 测试批量处理
    # processor.batch_process_documents("../data/documents")
```

### 3.2 向量知识库模块
向量知识库模块使用FAISS构建高效的向量存储，用于知识存储和检索。

```python
# src/vector_store.py
import os
import json
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from langchain.vectorstores import FAISS as LangChainFAISS
from langchain.embeddings import HuggingFaceEmbeddings

class VectorStore:
    """向量知识库模块，负责文档的向量化存储和检索"""
    def __init__(self, config):
        self.config = config
        self.embedding_model = None
        self.vector_store = None
        
        # 初始化嵌入模型
        self._init_embedding_model()
        
        # 如果向量库已存在，则加载
        if os.path.exists(self.config["vector_store"]["path"] + ".faiss"):
            self.load_vector_store()
    
    def _init_embedding_model(self):
        """初始化嵌入模型"""
        embedding_config = self.config["embedding_model"]
        
        # 使用HuggingFaceEmbeddings封装sentence-transformers模型
        self.embedding_model = HuggingFaceEmbeddings(
            model_name=embedding_config["name"],
            cache_folder=embedding_config["cache_dir"],
            model_kwargs={"device": embedding_config["device"]},
            encode_kwargs={"normalize_embeddings": True}  # 归一化嵌入向量
        )
        
        print(f"已加载嵌入模型: {embedding_config['name']}")
    
    def create_vector_store(self, documents):
        """创建向量知识库"""
        if not documents:
            raise ValueError("文档列表不能为空")
        
        try:
            # 使用LangChain的FAISS类创建向量库
            self.vector_store = LangChainFAISS.from_documents(
                documents=documents,
                embedding=self.embedding_model
            )
            
            print(f"已创建向量知识库，包含 {len(documents)} 个文档块")
            return self.vector_store
        except Exception as e:
            raise Exception(f"创建向量知识库失败: {str(e)}")
    
    def save_vector_store(self):
        """保存向量知识库到磁盘"""
        if not self.vector_store:
            raise ValueError("向量知识库不存在，请先创建")
        
        try:
            # 获取向量存储路径
            store_path = self.config["vector_store"]["path"]
            
            # 创建父目录（如果不存在）
            os.makedirs(os.path.dirname(store_path), exist_ok=True)
            
            # 保存向量库
            self.vector_store.save_local(store_path)
            
            print(f"向量知识库已保存到: {store_path}")
        except Exception as e:
            raise Exception(f"保存向量知识库失败: {str(e)}")
    
    def load_vector_store(self):
        """从磁盘加载向量知识库"""
        try:
            # 获取向量存储路径
            store_path = self.config["vector_store"]["path"]
            
            # 检查文件是否存在
            if not (os.path.exists(store_path + ".faiss") and os.path.exists(store_path + ".pkl")):
                print("向量知识库文件不存在，无法加载")
                return None
            
            # 加载向量库
            self.vector_store = LangChainFAISS.load_local(
                store_path,
                self.embedding_model,
                allow_dangerous_deserialization=True  # 允许反序列化
            )
            
            print(f"已加载向量知识库: {store_path}")
            return self.vector_store
        except Exception as e:
            print(f"加载向量知识库失败: {str(e)}")
            return None
    
    def search(self, query, k=None):
        """根据查询检索相关文档"""
        if not self.vector_store:
            raise ValueError("向量知识库不存在，请先创建或加载")
        
        # 使用配置中的top_k或传入的值
        top_k = k or self.config["vector_store"]["top_k"]
        
        try:
            # 执行相似性搜索
            results = self.vector_store.similarity_search(
                query=query,
                k=top_k
            )
            
            return results
        except Exception as e:
            raise Exception(f"搜索失败: {str(e)}")
    
    def batch_add_documents(self, documents):
        """批量添加文档到向量知识库"""
        if not self.vector_store:
            # 如果向量库不存在，则创建
            return self.create_vector_store(documents)
        
        try:
            # 添加文档到现有向量库
            self.vector_store.add_documents(documents)
            
            print(f"已向向量知识库添加 {len(documents)} 个文档块")
            return self.vector_store
        except Exception as e:
            raise Exception(f"添加文档失败: {str(e)}")

# 测试向量知识库模块
if __name__ == "__main__":
    # 加载配置
    with open("../config/config.json", "r", encoding="utf-8") as f:
        config = json.load(f)
    
    # 创建向量存储实例
    vector_store = VectorStore(config)
    
    # 创建示例文档（实际应用中应从文档处理模块获取）
    from langchain.schema import Document
    
    sample_docs = [
        Document(page_content="这是第一个示例文档的内容，关于人工智能的基础知识。", metadata={"source": "sample1.txt"}),
        Document(page_content="这是第二个示例文档的内容，讨论机器学习的各种算法。", metadata={"source": "sample2.txt"}),
        Document(page_content="这是第三个示例文档的内容，介绍深度学习的最新进展。", metadata={"source": "sample3.txt"})
    ]
    
    # 创建向量库
    vector_store.create_vector_store(sample_docs)
    
    # 保存向量库
    vector_store.save_vector_store()
    
    # 测试搜索
    results = vector_store.search("人工智能基础知识", k=2)
    print("搜索结果:")
    for i, result in enumerate(results):
        print(f"{i+1}. {result.page_content}")
        print(f"   来源: {result.metadata.get('source', '未知')}")
        print("---")
```

### 3.3 LLM推理模块
LLM推理模块负责加载本地大语言模型并执行推理。

```python
# src/llm_engine.py
import os
import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import pipeline

class LLMEngine:
    """LLM推理模块，负责加载大语言模型并执行推理"""
    def __init__(self, config):
        self.config = config
        self.model = None
        self.tokenizer = None
        self.pipe = None
        
        # 初始化模型
        self._init_model()
    
    def _init_model(self):
        """初始化大语言模型"""
        llm_config = self.config["llm"]
        
        try:
            # 清理CUDA缓存
            torch.cuda.empty_cache()
            
            print(f"开始加载模型: {llm_config['model_name']}")
            
            # 加载分词器
            self.tokenizer = AutoTokenizer.from_pretrained(
                llm_config["model_name"],
                cache_dir=llm_config["cache_dir"],
                use_fast=True,
                trust_remote_code=True
            )
            
            # 加载模型
            self.model = AutoModelForCausalLM.from_pretrained(
                llm_config["model_name"],
                cache_dir=llm_config["cache_dir"],
                device_map=llm_config["device_map"],
                torch_dtype=torch.float16,  # 使用半精度浮点数以节省显存
                low_cpu_mem_usage=True,     # 低CPU内存使用模式
                trust_remote_code=True
            )
            
            # 创建pipeline以简化推理过程
            self.pipe = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                torch_dtype=torch.float16,
                device_map=llm_config["device_map"]
            )
            
            print(f"模型 {llm_config['model_name']} 加载完成")
        except Exception as e:
            print(f"加载模型失败: {str(e)}")
            # 在加载失败的情况下，尝试使用CPU模式作为备选
            try:
                print("尝试以CPU模式加载模型...")
                self.tokenizer = AutoTokenizer.from_pretrained(
                    llm_config["model_name"],
                    cache_dir=llm_config["cache_dir"],
                    use_fast=True,
                    trust_remote_code=True
                )
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    llm_config["model_name"],
                    cache_dir=llm_config["cache_dir"],
                    device_map="cpu",  # 强制使用CPU
                    torch_dtype=torch.float32,
                    low_cpu_mem_usage=True,
                    trust_remote_code=True
                )
                
                self.pipe = pipeline(
                    "text-generation",
                    model=self.model,
                    tokenizer=self.tokenizer,
                    torch_dtype=torch.float32,
                    device_map="cpu"
                )
                
                print(f"模型 {llm_config['model_name']} 已以CPU模式加载")
            except Exception as e2:
                print(f"CPU模式加载也失败: {str(e2)}")
    
    def generate_text(self, prompt, max_new_tokens=None, temperature=None, **kwargs):
        """生成文本"""
        if not self.pipe:
            raise ValueError("模型未初始化，请先初始化模型")
        
        # 使用配置中的参数或传入的值
        llm_config = self.config["llm"]
        max_tokens = max_new_tokens or llm_config["max_new_tokens"]
        temp = temperature or llm_config["temperature"]
        
        try:
            # 清理CUDA缓存（仅在使用CUDA时）
            if torch.cuda.is_available() and self.config["llm"]["device_map"] != "cpu":
                torch.cuda.empty_cache()
            
            # 执行生成
            with torch.no_grad():
                outputs = self.pipe(
                    prompt,
                    max_new_tokens=max_tokens,
                    temperature=temp,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    **kwargs
                )
            
            # 提取生成的文本
            generated_text = outputs[0]["generated_text"][len(prompt):].strip()
            
            return generated_text
        except Exception as e:
            raise Exception(f"文本生成失败: {str(e)}")
    
    def generate_with_context(self, query, context_docs, **kwargs):
        """结合上下文生成回答"""
        # 构建带上下文的提示
        context = "\n".join([doc.page_content for doc in context_docs])
        
        # 构建提示模板（可以根据模型特点调整）
        prompt_template = """你是一个知识渊博的助手，根据提供的上下文信息回答用户的问题。

上下文信息:
{context}

用户问题:
{query}

请根据上下文信息，用中文简洁准确地回答用户的问题。如果上下文信息不足，请说明无法回答。"""
        
        prompt = prompt_template.format(context=context, query=query)
        
        # 调用生成函数
        return self.generate_text(prompt, **kwargs)

# 测试LLM推理模块
if __name__ == "__main__":
    # 加载配置
    with open("../config/config.json", "r", encoding="utf-8") as f:
        config = json.load(f)
    
    # 创建LLM引擎实例
    llm_engine = LLMEngine(config)
    
    # 测试简单生成
    if llm_engine.pipe:
        test_prompt = "请简要解释什么是人工智能？"
        print(f"\n测试提示: {test_prompt}")
        response = llm_engine.generate_text(test_prompt, max_new_tokens=200)
        print(f"模型回答: {response}")
        
        # 测试带上下文的生成
        from langchain.schema import Document
        context_docs = [
            Document(page_content="人工智能(AI)是计算机科学的一个分支，旨在创建能够执行通常需要人类智能的任务的系统。这些任务包括学习、推理、问题解决、感知和语言理解。", metadata={"source": "ai_intro.txt"}),
            Document(page_content="机器学习是人工智能的一个子集，它使计算机系统能够从数据中学习并随着经验的积累而改进，无需明确编程。", metadata={"source": "ml_intro.txt"})
        ]
        
        test_query = "人工智能和机器学习有什么关系？"
        print(f"\n测试问题: {test_query}")
        context_response = llm_engine.generate_with_context(test_query, context_docs)
        print(f"带上下文的回答: {context_response}")
```

### 3.4 RAG协调模块
RAG协调模块负责协调检索和生成过程，实现检索增强生成功能。

```python
# src/rag_coordinator.py
import os
import json
import time

class RAGCoordinator:
    """RAG协调模块，负责协调检索和生成过程"""
    def __init__(self, vector_store, llm_engine):
        self.vector_store = vector_store  # 向量存储实例
        self.llm_engine = llm_engine      # LLM引擎实例
        
        if not self.vector_store or not self.vector_store.vector_store:
            print("警告: 向量知识库未初始化，RAG功能可能受限")
        
        if not self.llm_engine or not self.llm_engine.pipe:
            print("警告: LLM引擎未初始化，无法生成回答")
    
    def rag_inference(self, query, k=None, **kwargs):
        """执行RAG推理过程"""
        if not self.vector_store or not self.vector_store.vector_store:
            raise ValueError("向量知识库未初始化，请先创建或加载向量库")
        
        if not self.llm_engine or not self.llm_engine.pipe:
            raise ValueError("LLM引擎未初始化，请先初始化模型")
        
        try:
            # 记录总时间
            total_start_time = time.time()
            
            # 1. 检索相关文档
            search_start_time = time.time()
            context_docs = self.vector_store.search(query, k=k)
            search_time = time.time() - search_start_time
            
            # 2. 生成回答
            generate_start_time = time.time()
            response = self.llm_engine.generate_with_context(query, context_docs, **kwargs)
            generate_time = time.time() - generate_start_time
            
            # 计算总时间
            total_time = time.time() - total_start_time
            
            # 准备结果
            result = {
                "query": query,
                "response": response,
                "context_docs": [
                    {"content": doc.page_content, "metadata": doc.metadata} 
                    for doc in context_docs
                ],
                "metrics": {
                    "search_time": search_time,
                    "generate_time": generate_time,
                    "total_time": total_time,
                    "context_docs_count": len(context_docs)
                }
            }
            
            return result
        except Exception as e:
            raise Exception(f"RAG推理失败: {str(e)}")
    
    def answer_question(self, query, k=None, return_details=False, **kwargs):
        """回答用户问题，是RAG推理的简化接口"""
        try:
            # 执行RAG推理
            result = self.rag_inference(query, k=k, **kwargs)
            
            if return_details:
                return result["response"], result
            else:
                return result["response"]
        except Exception as e:
            error_msg = f"回答问题失败: {str(e)}"
            print(error_msg)
            if return_details:
                return error_msg, {"error": error_msg}
            else:
                return error_msg
    
    def batch_answer_questions(self, queries, k=None, **kwargs):
        """批量回答问题"""
        results = []
        
        for i, query in enumerate(queries):
            print(f"处理问题 {i+1}/{len(queries)}: {query}")
            response, details = self.answer_question(query, k=k, return_details=True, **kwargs)
            
            results.append({
                "query": query,
                "response": response,
                "details": details
            })
        
        return results
    
    def evaluate_rag_performance(self, test_qa_pairs, k=None):
        """评估RAG系统的性能"""
        if not test_qa_pairs:
            raise ValueError("测试问答对不能为空")
        
        results = []
        total_time = 0
        
        print(f"开始评估RAG性能，共 {len(test_qa_pairs)} 个测试用例")
        
        for i, qa_pair in enumerate(test_qa_pairs):
            query = qa_pair["question"]
            expected_answer = qa_pair.get("expected_answer", "")
            
            print(f"评估用例 {i+1}/{len(test_qa_pairs)}: {query}")
            
            # 记录开始时间
            start_time = time.time()
            
            # 执行RAG推理
            try:
                response, details = self.answer_question(query, k=k, return_details=True)
                
                # 计算时间
                processing_time = time.time() - start_time
                total_time += processing_time
                
                # 简单评估（实际应用中可能需要更复杂的评估指标）
                relevance_score = 1 if expected_answer in response else 0  # 简单的相关性评估
                
                results.append({
                    "query": query,
                    "response": response,
                    "expected_answer": expected_answer,
                    "processing_time": processing_time,
                    "relevance_score": relevance_score,
                    "details": details
                })
                
            except Exception as e:
                print(f"评估失败: {str(e)}")
                results.append({
                    "query": query,
                    "response": f"评估失败: {str(e)}",
                    "expected_answer": expected_answer,
                    "processing_time": 0,
                    "relevance_score": 0,
                    "error": str(e)
                })
        
        # 计算总体性能指标
        avg_time = total_time / len(results) if results else 0
        avg_relevance = sum(r.get("relevance_score", 0) for r in results) / len(results) if results else 0
        
        evaluation_summary = {
            "total_cases": len(results),
            "avg_processing_time": avg_time,
            "avg_relevance_score": avg_relevance,
            "results": results
        }
        
        print(f"评估完成。平均处理时间: {avg_time:.2f}秒，平均相关度评分: {avg_relevance:.2f}")
        
        return evaluation_summary

# 测试RAG协调模块
if __name__ == "__main__":
    # 加载配置
    with open("../config/config.json", "r", encoding="utf-8") as f:
        config = json.load(f)
    
    # 导入必要的模块
    from vector_store import VectorStore
    from llm_engine import LLMEngine
    from langchain.schema import Document
    
    # 创建向量存储实例
    vector_store = VectorStore(config)
    
    # 创建示例文档（实际应用中应从文档处理模块获取）
    sample_docs = [
        Document(page_content="人工智能(AI)是计算机科学的一个分支，旨在创建能够执行通常需要人类智能的任务的系统。这些任务包括学习、推理、问题解决、感知和语言理解。", metadata={"source": "ai_intro.txt"}),
        Document(page_content="机器学习是人工智能的一个子集，它使计算机系统能够从数据中学习并随着经验的积累而改进，无需明确编程。", metadata={"source": "ml_intro.txt"}),
        Document(page_content="深度学习是机器学习的一个分支，它使用多层神经网络来模拟数据的高级抽象。", metadata={"source": "dl_intro.txt"})
    ]
    
    # 创建向量库（如果不存在）
    if not vector_store.vector_store:
        vector_store.create_vector_store(sample_docs)
    
    # 创建LLM引擎实例
    llm_engine = LLMEngine(config)
    
    # 创建RAG协调器
    if llm_engine.pipe:
        rag_coordinator = RAGCoordinator(vector_store, llm_engine)
        
        # 测试RAG回答
        test_query = "人工智能、机器学习和深度学习之间有什么关系？"
        print(f"\n测试问题: {test_query}")
        response = rag_coordinator.answer_question(test_query)
        print(f"RAG回答: {response}")
        
        # 测试带详细信息的回答
        response, details = rag_coordinator.answer_question(test_query, return_details=True)
        print(f"\n详细信息:")
        print(f"搜索时间: {details['metrics']['search_time']:.2f}秒")
        print(f"生成时间: {details['metrics']['generate_time']:.2f}秒")
        print(f"使用的上下文文档数: {details['metrics']['context_docs_count']}")
```

## 4. 用户界面实现

### 4.1 使用Gradio创建Web界面
我们将使用Gradio创建一个简单直观的Web界面，让用户可以与个人知识库助手进行交互。

```python
# src/ui.py
import os
import json
import gradio as gr
from datetime import datetime

class KnowledgeAssistantUI:
    """个人知识库助手的用户界面模块"""
    def __init__(self, rag_coordinator, document_processor, config):
        self.rag_coordinator = rag_coordinator  # RAG协调器实例
        self.document_processor = document_processor  # 文档处理器实例
        self.config = config  # 配置
        self.interface = None  # Gradio界面
        
    def _create_ui(self):
        """创建Gradio界面"""
        with gr.Blocks(title="个人知识库助手", theme=gr.themes.Soft()) as interface:
            # 添加标题和描述
            gr.Markdown("# 个人知识库助手")
            gr.Markdown("基于RAG技术的本地知识库问答系统")
            
            # 对话历史状态变量
            chat_history = gr.State([])
            
            with gr.Row():
                with gr.Column(scale=1):
                    # 左侧面板：上传文档和控制选项
                    gr.Markdown("## 文档管理")
                    
                    # 文件上传组件
                    file_upload = gr.File(
                        label="上传文档",
                        file_types=[".pdf", ".docx", ".md", ".txt"],
                        file_count="multiple"
                    )
                    
                    # 上传按钮
                    upload_button = gr.Button("上传并处理文档")
                    
                    # 处理状态显示
                    process_status = gr.Textbox(
                        label="处理状态",
                        placeholder="等待上传...",
                        interactive=False
                    )
                    
                    # 检索参数控制
                    gr.Markdown("## 检索参数")
                    top_k = gr.Slider(
                        minimum=1, maximum=10, value=self.config["vector_store"]["top_k"],
                        step=1, label="检索文档数量 (k)", info="控制从知识库检索的文档数量"
                    )
                    
                    # 生成参数控制
                    gr.Markdown("## 生成参数")
                    max_tokens = gr.Slider(
                        minimum=100, maximum=2000, value=self.config["llm"]["max_new_tokens"],
                        step=100, label="最大生成长度", info="控制回答的最大长度"
                    )
                    temperature = gr.Slider(
                        minimum=0.1, maximum=1.5, value=self.config["llm"]["temperature"],
                        step=0.1, label="生成温度", info="控制回答的随机性，值越高越随机"
                    )
                    
                    # 清空对话按钮
                    clear_button = gr.Button("清空对话历史")
                    
                with gr.Column(scale=3):
                    # 右侧面板：对话区域
                    gr.Markdown("## 对话区域")
                    
                    # 聊天界面
                    chatbot = gr.Chatbot(label="个人知识库助手")
                    
                    # 用户输入框
                    user_input = gr.Textbox(
                        label="请输入您的问题",
                        placeholder="请问..."
                    )
                    
                    # 提交按钮
                    submit_button = gr.Button("发送", variant="primary")
                    
                    # 绑定输入框的回车事件
                    user_input.submit(
                        self._respond_to_query, 
                        inputs=[user_input, chatbot, chat_history, top_k, max_tokens, temperature],
                        outputs=[user_input, chatbot, chat_history]
                    )
                    
                    # 绑定提交按钮的点击事件
                    submit_button.click(
                        self._respond_to_query, 
                        inputs=[user_input, chatbot, chat_history, top_k, max_tokens, temperature],
                        outputs=[user_input, chatbot, chat_history]
                    )
            
            # 绑定文件上传和处理事件
            upload_button.click(
                self._process_uploaded_files, 
                inputs=[file_upload],
                outputs=[process_status]
            )
            
            # 绑定清空对话事件
            clear_button.click(
                self._clear_chat, 
                inputs=[],
                outputs=[chatbot, chat_history]
            )
        
        return interface
    
    def _process_uploaded_files(self, files):
        """处理上传的文件"""
        if not files:
            return "请先选择要上传的文件"
        
        try:
            # 创建临时目录保存上传的文件
            temp_dir = "../data/documents/uploaded"
            os.makedirs(temp_dir, exist_ok=True)
            
            processed_chunks = []
            
            for file in files:
                # 保存文件到临时目录
                file_path = os.path.join(temp_dir, file.name)
                
                # 读取文件内容并保存
                with open(file_path, "wb") as f:
                    f.write(file.read())
                
                # 处理文件
                chunks = self.document_processor.process_document(file_path)
                processed_chunks.extend(chunks)
                
            # 将处理后的文档添加到向量库
            self.rag_coordinator.vector_store.batch_add_documents(processed_chunks)
            
            # 保存更新后的向量库
            self.rag_coordinator.vector_store.save_vector_store()
            
            return f"成功处理并添加了 {len(files)} 个文件，共 {len(processed_chunks)} 个文档块到知识库"
        except Exception as e:
            return f"处理文件失败: {str(e)}"
    
    def _respond_to_query(self, user_message, chatbot, history, top_k, max_tokens, temperature):
        """响应用户查询"""
        if not user_message.strip():
            return gr.update(value=""), chatbot, history
        
        try:
            # 添加用户消息到聊天历史
            chatbot.append((user_message, None))
            history.append({"role": "user", "content": user_message})
            
            # 使用RAG生成回答
            response = self.rag_coordinator.answer_question(
                user_message,
                k=top_k,
                max_new_tokens=max_tokens,
                temperature=temperature
            )
            
            # 更新聊天历史
            chatbot[-1] = (user_message, response)
            history.append({"role": "assistant", "content": response})
            
            # 保存对话历史（可选）
            self._save_chat_history(history)
            
            return gr.update(value=""), chatbot, history
        except Exception as e:
            error_msg = f"生成回答时出错: {str(e)}"
            chatbot[-1] = (user_message, error_msg)
            return gr.update(value=""), chatbot, history
    
    def _clear_chat(self):
        """清空聊天历史"""
        return [], []
    
    def _save_chat_history(self, history):
        """保存对话历史到文件（可选功能）"""
        try:
            # 限制历史记录长度
            max_history = self.config["ui"]["max_history"]
            if len(history) > max_history:
                history = history[-max_history:]
            
            # 创建历史记录目录
            history_dir = "../data/chat_history"
            os.makedirs(history_dir, exist_ok=True)
            
            # 生成文件名（基于当前日期时间）
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            history_path = os.path.join(history_dir, f"chat_history_{timestamp}.json")
            
            # 保存历史记录
            with open(history_path, "w", encoding="utf-8") as f:
                json.dump(history, f, ensure_ascii=False, indent=2)
            
        except Exception as e:
            print(f"保存对话历史失败: {str(e)}")
    
    def launch(self, share=False, debug=False):
        """启动Web界面"""
        # 创建界面
        self.interface = self._create_ui()
        
        # 启动界面
        self.interface.launch(
            share=share,   # 是否生成公网可访问的链接
            debug=debug,   # 是否启用调试模式
            inline=False,  # 是否内嵌在Jupyter中
            server_name="0.0.0.0",  # 允许从任何IP访问
            server_port=7860  # 服务端口
        )

# 测试用户界面模块
if __name__ == "__main__":
    # 这是一个示例，实际使用时应从主程序中初始化
    print("请从主程序启动用户界面")
```

## 5. 主程序实现

### 5.1 整合所有模块
下面是主程序，它将整合所有模块并启动个人知识库助手应用。

```python
# main.py
import os
import json
import logging

# 配置日志
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger("personal_knowledge_assistant")

# 导入自定义模块
try:
    from src.document_processor import DocumentProcessor
    from src.vector_store import VectorStore
    from src.llm_engine import LLMEngine
    from src.rag_coordinator import RAGCoordinator
    from src.ui import KnowledgeAssistantUI
except ImportError as e:
    logger.error(f"导入模块失败: {e}")
    logger.info("请确保您在正确的目录下运行程序")
    exit(1)

class PersonalKnowledgeAssistant:
    """个人知识库助手主程序"""
    def __init__(self):
        self.config = None
        self.document_processor = None
        self.vector_store = None
        self.llm_engine = None
        self.rag_coordinator = None
        self.ui = None
        
        # 初始化系统
        self._initialize()
    
    def _initialize(self):
        """初始化系统组件"""
        try:
            # 1. 加载配置
            self._load_config()
            
            # 2. 初始化文档处理器
            self._init_document_processor()
            
            # 3. 初始化向量存储
            self._init_vector_store()
            
            # 4. 初始化LLM引擎
            self._init_llm_engine()
            
            # 5. 初始化RAG协调器
            self._init_rag_coordinator()
            
            logger.info("个人知识库助手初始化完成")
        except Exception as e:
            logger.error(f"初始化失败: {e}")
            raise
    
    def _load_config(self):
        """加载配置文件"""
        config_path = "config/config.json"
        
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"配置文件不存在: {config_path}")
        
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                self.config = json.load(f)
            
            logger.info(f"已加载配置文件: {config_path}")
        except json.JSONDecodeError as e:
            raise ValueError(f"配置文件格式错误: {e}")
    
    def _init_document_processor(self):
        """初始化文档处理器"""
        self.document_processor = DocumentProcessor(self.config)
        logger.info("文档处理器初始化完成")
    
    def _init_vector_store(self):
        """初始化向量存储"""
        self.vector_store = VectorStore(self.config)
        logger.info("向量存储初始化完成")
    
    def _init_llm_engine(self):
        """初始化LLM引擎"""
        self.llm_engine = LLMEngine(self.config)
        logger.info("LLM引擎初始化完成")
    
    def _init_rag_coordinator(self):
        """初始化RAG协调器"""
        self.rag_coordinator = RAGCoordinator(self.vector_store, self.llm_engine)
        logger.info("RAG协调器初始化完成")
    
    def init_ui(self):
        """初始化用户界面"""
        self.ui = KnowledgeAssistantUI(self.rag_coordinator, self.document_processor, self.config)
        logger.info("用户界面初始化完成")
    
    def start(self, with_ui=True, share=False):
        """启动个人知识库助手"""
        try:
            logger.info("开始启动个人知识库助手")
            
            # 检查是否需要启动UI
            if with_ui:
                # 初始化UI
                self.init_ui()
                
                # 启动Web界面
                logger.info("正在启动Web界面...")
                self.ui.launch(share=share)
            else:
                # 仅启动核心服务（无UI）
                logger.info("个人知识库助手核心服务已启动")
                
                # 示例：直接使用RAG服务
                if hasattr(self, 'rag_coordinator') and self.rag_coordinator:
                    # 这里可以添加命令行交互或其他非UI交互方式
                    logger.info("您可以通过编程方式使用RAG协调器进行问答")
            
        except Exception as e:
            logger.error(f"启动失败: {e}")
            raise
    
    def batch_process_documents(self, directory_path):
        """批量处理目录中的文档"""
        if not self.document_processor:
            raise ValueError("文档处理器未初始化")
        
        try:
            logger.info(f"开始批量处理目录 {directory_path} 中的文档")
            
            # 处理文档
            chunks = self.document_processor.batch_process_documents(directory_path)
            
            if chunks:
                # 添加到向量库
                self.vector_store.batch_add_documents(chunks)
                
                # 保存向量库
                self.vector_store.save_vector_store()
                
                logger.info(f"成功处理 {len(chunks)} 个文档块并添加到知识库")
                return len(chunks)
            else:
                logger.warning("未找到或处理任何文档")
                return 0
        except Exception as e:
            logger.error(f"批量处理文档失败: {e}")
            raise
    
    def query(self, question, **kwargs):
        """直接查询知识库"""
        if not self.rag_coordinator:
            raise ValueError("RAG协调器未初始化")
        
        try:
            logger.info(f"处理查询: {question}")
            
            # 使用RAG生成回答
            response = self.rag_coordinator.answer_question(question, **kwargs)
            
            logger.info("查询处理完成")
            return response
        except Exception as e:
            logger.error(f"查询处理失败: {e}")
            raise

# 主程序入口
if __name__ == "__main__":
    try:
        # 创建个人知识库助手实例
        assistant = PersonalKnowledgeAssistant()
        
        # 可选：批量处理已有文档
        # documents_dir = "data/documents"
        # if os.path.exists(documents_dir):
        #     assistant.batch_process_documents(documents_dir)
        
        # 启动助手（带UI）
        assistant.start(with_ui=True, share=False)  # 设置share=True可以生成公网链接
        
    except KeyboardInterrupt:
        logger.info("程序已被用户中断")
    except Exception as e:
        logger.error(f"程序运行出错: {e}")
        print(f"程序运行出错: {e}")
```

## 6. 部署与运行指南

### 6.1 安装依赖
在运行项目之前，请确保已安装所有必要的依赖。可以使用以下命令安装：

```bash
# 安装基础依赖
pip install -r requirements.txt

# 或者手动安装主要依赖
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers accelerate sentencepiece langchain pypdf python-docx markdown faiss-cpu sentence-transformers gradio
```

### 6.2 运行项目
安装完成后，可以使用以下命令运行项目：

```bash
# 在项目根目录下运行
python main.py
```

运行后，程序会初始化各个模块，并在浏览器中打开Web界面（默认地址为 http://localhost:7860）。

### 6.3 首次使用指南
首次使用个人知识库助手时，建议按照以下步骤操作：

1. **准备文档**：将要添加到知识库的文档（PDF、Word、Markdown、文本文件）准备好
2. **上传文档**：在Web界面的左侧面板中，点击"上传文档"按钮选择并上传准备好的文档
3. **处理文档**：点击"上传并处理文档"按钮，系统会自动处理文档并添加到知识库
4. **开始问答**：在右侧的对话区域输入问题，点击"发送"按钮获取基于知识库的回答
5. **调整参数**：根据需要调整检索参数和生成参数，以获得更好的回答质量

### 6.4 在RTX 3060 12G上的优化建议
为了在RTX 3060 12G显卡上获得最佳性能，建议进行以下优化：

1. **使用量化模型**：选择4位或8位量化的模型，如TheBloke提供的GPTQ量化模型
2. **调整batch size**：将批处理大小设置为1，以减少显存占用
3. **限制生成长度**：将最大生成长度设置在512-1024之间
4. **减少检索文档数量**：将检索文档数量设置在3-5之间
5. **关闭不必要的程序**：在运行期间关闭其他占用显存的程序

## 7. 常见问题与解决方案

### 7.1 内存或显存不足
**问题**：运行过程中出现CUDA内存不足或系统内存不足的错误。
**解决方案**：
- 使用更小或更激进量化的模型
- 减少检索的文档数量
- 限制生成的最大token数
- 增加系统虚拟内存（对于Windows系统）
- 尝试使用CPU模式运行（虽然速度会慢很多）

### 7.2 模型下载失败
**问题**：下载预训练模型时出现连接错误或下载中断。
**解决方案**：
- 检查网络连接是否正常
- 考虑使用VPN或代理服务器
- 手动下载模型文件并放置到相应的缓存目录
- 使用Hugging Face的`huggingface-cli download`命令下载

### 7.3 文档处理错误
**问题**：上传和处理文档时出现错误。
**解决方案**：
- 检查文档格式是否受支持（目前支持PDF、Word、Markdown、文本文件）
- 确保文档没有损坏
- 对于大文件，考虑分割成较小的文件后再上传
- 检查文件权限，确保程序有读取权限

### 7.4 回答质量不佳
**问题**：生成的回答不准确或不相关。
**解决方案**：
- 增加检索的文档数量
- 调整生成温度参数（通常较低的温度会产生更确定的回答）
- 改进提问方式，使问题更明确、更具体
- 添加更多相关领域的文档到知识库
- 考虑使用更适合特定领域的模型

### 7.5 Web界面访问问题
**问题**：无法访问Web界面或界面加载缓慢。
**解决方案**：
- 检查浏览器是否兼容（推荐使用Chrome、Firefox或Edge）
- 确认服务是否正在运行（查看控制台输出）
- 尝试刷新页面或清除浏览器缓存
- 检查防火墙设置，确保端口7860已开放

## 8. 扩展与进阶方向

### 8.1 功能扩展
1. **支持更多文档格式**：添加对PPT、Excel、网页等格式的支持
2. **多模态支持**：集成图像、音频等多种模态的处理能力
3. **自动摘要生成**：为长文档自动生成摘要
4. **知识图谱整合**：结合知识图谱技术增强检索和推理能力
5. **个性化推荐**：根据用户偏好和历史行为提供个性化知识推荐

### 8.2 性能优化
1. **模型微调**：在特定领域数据集上微调预训练模型
2. **分布式部署**：在多GPU或多机器环境中分布式部署系统
3. **索引优化**：使用更高效的向量索引结构和检索算法
4. **缓存机制**：添加查询和回答缓存，减少重复计算
5. **异步处理**：实现异步文档处理和查询响应机制

### 8.3 安全与隐私
1. **数据加密**：对存储的文档和向量数据进行加密
2. **访问控制**：添加用户认证和授权机制
3. **隐私保护**：实现差分隐私等技术保护用户数据
4. **敏感信息过滤**：自动检测和过滤敏感信息

### 8.4 集成与自动化
1. **API接口**：提供RESTful API接口，方便与其他系统集成
2. **自动化工作流**：实现文档定期更新和重新索引的自动化工作流
3. **第三方工具集成**：与Notion、Obsidian、OneNote等工具集成
4. **语音交互**：添加语音识别和合成功能，支持语音交互

## 9. 总结

个人知识库助手是一个强大的工具，可以帮助用户有效地管理和利用个人知识资源。本项目通过整合文档处理、向量存储、本地LLM和RAG技术，构建了一个完整的端到端解决方案。

在本项目中，我们学习了如何：
1. 设计和实现模块化的系统架构
2. 处理各种格式的文档并构建高效的向量知识库
3. 部署和优化本地大语言模型
4. 实现检索增强生成（RAG）功能
5. 创建直观的用户界面进行交互

通过这个项目，我们不仅掌握了AI应用开发的核心技术，还学会了如何将理论知识应用到实际问题中。随着技术的不断发展，个人知识库助手的功能和性能也将不断提升，为用户提供更加智能、高效的知识管理体验。

希望这个项目能够帮助您更好地组织和利用个人知识，提高工作和学习效率！